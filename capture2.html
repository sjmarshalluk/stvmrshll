<!doctype html>
<html>
  <head>
        <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-QQTC3NEETR"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-QQTC3NEETR');
</script>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <!-- Use the title from a page's frontmatter if it has one -->
    <meta name="apple-mobile-web-app-capable" content="yes" /> 
    <meta name="apple-touch-fullscreen" content="yes" />
    <title>Steve Marshall</title>
    <link href="./stylesheets/resetx.css" rel="stylesheet" />
    <link href="./stylesheets/gridsx.css" rel="stylesheet" />
    <link href="./stylesheets/typographyx.css" rel="stylesheet" />
    <link href="./stylesheets/site.css" rel="stylesheet" />

    <link href="https://fonts.googleapis.com/css?family=Rubik:400,500,500i,700&display=swap" rel="stylesheet">

   <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Schibsted+Grotesk:ital,wght@0,400;0,500;0,600;0,700;0,800;1,400;1,500;1,600;1,700;1,800&display=swap" rel="stylesheet">
    <link href="./images/favicon.png" rel="icon" type="image/png" />
  </head>
  <body>


  

  <div class="sidebar">
    <p class="meta">stv mrshll</p>
    <ul>
      <li><a href="outside-onboarding.html">Defining a unified cross-product onboarding strategy<span>Outside, 2025</span></a></li>
      <li><a href="outside-design-system.html">Establishing paradigms for a multi-product design system<span>Outside, 2024</span></a></li>
      <li><a class="active" href="capture.html">Developing emerging tech by automating XR capture<span>Meta, 2022</span></a></li>
      <li><a href="design-club.html">Creating a curriculum to teach design thinking as a life skill<span>Design Club, 2018</span></a></li>
    </ul>

  </div>
  

  <div class="content-wrapper">
    <div class="content-text">

      <section class="content-section">
        <div class="section-text">
          <p class="meta">Design Lead · Meta Reality Labs · 2022</p>
          <h1>Automating mixed-reality environment capture</h1>

          <p><strong>Summary:</strong> Mixed Reality on Quest needed reliable 3D maps of users’ rooms, but capture tech and requirements were immature. I led the design of the capture model, aligned teams on what data we needed, explored feasible UX patterns, and defined a three-step capture flow that set the foundation for future automated room scanning.</p>

      
          <h2>The Challenge</h2>

          <p>
          Mixed Reality only works if the headset can build a watertight, accurate 3D model of a user's room. 
          Early Quest devices relied on a manual, low-fidelity setup flow that wasn't designed to support MR physics, occlusion, or object interactions.
          </p>

          <p>
          Meta needed a path toward <strong>automated room scanning</strong>, but the organization lacked clarity on three fronts:
          </p>

          <ul class="bullets">
            <li>Detection technology was still immature and unreliable.</li>
            <li>Teams held conflicting assumptions about what "capture" should produce.</li>
            <li>No one had defined the system requirements or workflow needed to support MR at platform scale.</li>
          </ul>

          <p>
          My role was to establish a unified model for environment capture and align design, PM, and engineering on a viable approach that could evolve from manual labeling → assisted capture → future automated scanning.
          </p>
        </div>
        <div class="section-media">
         
        </div>
      </section>

      <section class="content-section">
        <div class="section-text">
          <h2>Defining What Capture Must Produce</h2>

          <p>
          I led cross-functional workshops with SLAM engineers, perception researchers, MR experience teams, and external developers to clarify what the 
          capture system needed to output—independent of UI or technical preferences.
          </p>

          <p>Through synthesis of MR use cases and tech constraints, we defined capture requirements:</p>

          <ul class="bullets">
            <li>A <strong>watertight room volume</strong> to support physics and boundaries.</li>
            <li>Detectable <strong>planes</strong> (walls, floors, ceilings).</li>
            <li><strong>Object volumes</strong> and categories for interaction models.</li>
            <li>Optional <strong>high-fidelity meshes</strong> for advanced experiences.</li>
          </ul>

          <p>
          This became the first shared, platform-level definition of what "capture" needed to produce, shifting conversations away 
          from feature debates toward <strong>functional MR requirements</strong>.
          </p>


          <h2>Exploring the solution space</h2>

          <p>
          With requirements set, I mapped a spectrum of viable UX approaches—balancing user expectations with early technical constraints.
          </p>

          <ul class="numbers">
            <li><strong>Room → guided objects</strong>: Most reliable, higher user effort.</li>
            <li><strong>Room → automatic object detection</strong>: Better UX, moderate tech risk.</li>
            <li><strong>Room + objects simultaneously</strong>: Long-term ideal, not yet feasible.</li>
          </ul>

          <p>
          Each model was evaluated for detection feasibility, error-recovery patterns, clarity of user mental model, engineering cost, and 
          its pathway toward future automation. This produced the first strategic roadmap for how capture could evolve over multiple hardware cycles.
          </p>
        </div>
        <div class="section-media">
          <img src="./images/capture/new/docs1.png" alt="Capture requirements bullets" loading="lazy" />
       
          <img src="./images/capture/capture-flows.png" alt="Capture flow options" loading="lazy" />
        </div>
      </section>


      <section class="content-section">
        <div class="section-text">
          <h2>Designing for Imperfect Technology</h2>

          <p>
          Because perception systems were still developing, the UX needed robust fallback paths for real-world failures, including:
          </p>

          <ul class="bullets">
            <li>Successful detection.</li>
            <li>Partial or incorrect detection.</li>
            <li>Complete failure to detect surfaces.</li>
            <li>Open-plan spaces with no natural boundaries.</li>
          </ul>

          <p>
          I designed flows that allowed users to correct errors, add missing geometry, and maintain a coherent room model even when 
          automation struggled—ensuring predictable MR behavior despite tech limitations.
          </p>

        <h2>Prototyping Ahead of the Tech</h2>
        <p>Because the underlying detection system was still in development, we couldn't rely on functional prototypes to validate our UX. To unblock progress, I partnered with a small engineering team to define and build a <strong>fake-data prototyping environment</strong> that simulated detection accuracy and errors based on current predictions.</p>
        <p>This allowed us to run realistic UX tests, compare flows, and make confident recommendations before the core tech was fully available.</p>
        </div>

      </section>

     <section class="content-section">
       <div class="section-text">
       <h2>Final Direction: A Three-Step Capture Model</h2>

       <p>
       Through structured trade-off reviews with engineering and product leadership, we aligned on a launchable sequence that balanced 
       usability, clarity, and technical feasibility:
       </p>

       <ul class="numbers">
         <li><strong>Capture the room</strong>: Establish watertight bounds and correct inaccuracies.</li>
         <li><strong>Detect planes and object volumes</strong>: Automate surfaces and major objects.</li>
         <li><strong>Optional mesh refinement</strong>: Add fidelity where experiences require it.</li>
       </ul>

       <p>
       This framework created a scalable path toward fully automated capture while ensuring the initial experience was stable, recoverable, and intuitive.
       </p>

       <h2>Impact</h2>

<p>
This project laid the conceptual and structural foundations for Meta’s modern environment-capture pipeline.
</p>

<p>
After I left, the core principles defined here—room → planes → objects, with optional mesh refinement—were adopted 
into the <strong>Meta Spatial Scanner</strong>, the official reference implementation used across Quest devices. 
It now appears in Meta’s developer documentation:
<a href="https://developers.meta.com/horizon/documentation/spatial-sdk/spatial-sdk-scanner-overview/" target="_blank">Spatial Scanner Overview</a>.
</p>

<p>
The same model underpins the scanning capabilities demonstrated in the <strong>Meta Connect 2025 Developer Keynote</strong>, 
where Meta showcased room scanning, mesh reconstruction, and spatial understanding built on top of these foundations.
</p>

<p>
This work provided:</p>

<ul class="bullets">
  <li>The first unified definition of capture requirements used by XR, SLAM, and SDK teams.</li>
  <li>A stable spatial data model that unblocked MR experience development across the Quest ecosystem.</li>
  <li>A scalable architectural path from manual capture to assisted workflows to future full automation.</li>
  <li>Prototype environments and decision frameworks still referenced in today’s Spatial SDK evolution.</li>
</ul>

<p>
In short, this project transformed capture from an experimental feature into a <strong>platform-level capability</strong> now embedded 
in Quest devices, the Spatial SDK, and the developer tooling ecosystem.
</p>
       </div>
       <div class="section-media">

         <img src="./images/capture/flow-ui.png" alt="Final capture UI" loading="lazy" />
         <img src="./images/capture/mockups.png" alt="Capture mockups" loading="lazy" />
       </div>
     </section>


    </div>

    
  </div>

</div>

  </body>
</html>
