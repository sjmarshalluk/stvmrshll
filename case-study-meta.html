<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Steve Marshall | Staff Product Designer</title>
    <link href="./stylesheets/site.css" rel="stylesheet" />
    <link href="./images/favicon.png" rel="icon" type="image/png" />

       <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@500&display=swap" rel="stylesheet">

<link href="https://fonts.googleapis.com/css2?family=Schibsted+Grotesk:ital,wght@0,400;0,500;0,600;0,700;0,800;1,400;1,500;1,600;1,700;1,800&display=swap" rel="stylesheet">


<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-QQTC3NEETR"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-QQTC3NEETR');
</script>
</head>
<body>

    <main class="case-study-container">

        <div class="case-study-section">
            <div class="section-text">
                <div class="case-study-nav">
                    <a href="index.html" class="base-name">stvmrshll</a>/<a class="base-name" href="/work">work</a><span class="path-text">/meta-vr-capture</span>
                </div>

                <p class="intro" data-figure="fig-01">
                   Defined a human-in-the-loop spatial capture model to handle unreliable automation, enabling robust environment scanning and laying the foundation for Meta’s Spatial Scanner and SDK.
                </p>

                <div class="memo-meta" style="margin-top: 2rem;">
                    <div class="meta-item">
                        <strong>Role</strong>
                        <span>Design Lead</span>
                    </div>
                    <div class="meta-item">
                        <strong>Context</strong>
                        <span>Meta Quest Platform</span>
                    </div>
                </div>

                <h3 data-figure="fig-02">Designing for Imperfect Eyes</h3>
                <p>
                    Mixed Reality (MR) depends on the headset understanding the physics of the room. Early Quest devices relied on users manually "drawing" their walls—a high-friction process that resulted in poor data.
                </p>
                <p>
                    The business goal was "Automated Scanning," but the technology wasn't ready. The computer vision (SLAM) teams were struggling with reliability. They needed a design strategy that could handle the ambiguity of the real world: open floor plans, messy rooms, and occlusion.
                </p>

                <h3 data-figure="fig-03">The Hybrid Capture Model</h3>
                <p>
                    We couldn't rely on full automation because the tech wasn't mature. I defined a "Human-in-the-Loop" model: The machine suggests the geometry, and the human confirms or corrects it.
                </p>
                <p>
                    I broke the capture process down into four distinct primitives, allowing us to mix and match manual vs. automated steps depending on the hardware generation:
                </p>
                <ol style="margin-left: 1.5rem; margin-bottom: 2rem;">
                    <li><strong>Room Layout:</strong> (Walls, Ceiling, Floor)</li>
                    <li><strong>Architectural Features:</strong> (Windows, Doors)</li>
                    <li><strong>Object Volumes:</strong> (Couches, Tables - for collision)</li>
                    <li><strong>Object Meshes:</strong> (High fidelity visuals)</li>
                </ol>

                <h3 data-figure="fig-04">Designing for Failure</h3>
                <p>
                    The most critical part of this work was not the "Happy Path," but the "Correction Path." What happens when the headset misses a wall? What happens if it thinks a couch is a table?
                </p>
                <p>
                    I designed a comprehensive fallback logic that allowed users to seamlessly take over when the algorithms failed.
                </p>

                <h3 data-figure="fig-05">From Experiment to SDK</h3>
                <p>
                    This work moved environment capture from a research demo to a shipping platform capability.
                </p>
                <ul>
                    <li><strong>The Meta Spatial Scanner:</strong> The model I defined (Room → Planes → Objects) became the reference implementation for the <em>Meta Spatial Scanner</em>, now a core part of the Quest OS.</li>
                    <li><strong>Developer Standard:</strong> This framework is now embedded in the Spatial SDK, defining how thousands of third-party developers interact with user environments.</li>
                    <li><strong>Unblocking Progression:</strong> By providing an experience that could handle "noisy" data, we enabled progression before the machine learning models were 100% perfect, ensuring Quest 3 could be launched on time.</li>
                </ul>

                <footer>
                <a href="case-study-maps.html">← Previous: Maps Platform</a>
                <a href="case-study-bysm.html">Next: LiDAR Metaverse →</a>
            </footer>

            </div>
            <div class="section-images">
                <div class="figure-container full-bleed" data-figure-id="fig-01">
                    <div class="figure-content">
                        <img src="images/capture/ui.png" alt="Key Requirements for MR Capture">
                    </div>
                    <div class="caption">
                    </div>
                </div>
                <div class="figure-container bg-dark" data-figure-id="fig-02">
                    <div class="figure-content">
                        <img src="images/capture/docs1.png" alt="Key Requirements for MR Capture">
                    </div>
                    <div class="caption">
                        Defining the required output: watertight geometry for physics, semantic labels, and granular meshes for occlusion.
                    </div>
                </div>
                <div class="figure-container bg-dark full-bleed" data-figure-id="fig-03">
                    <div class="figure-content">
                        <img src="images/capture/docs1.png" alt="The Component Model">
                    </div>
                    <div class="caption">
                        Breaking the environment down into layers allowed us to ship "Room Layout" first while "Object Detection" was still in R&D.
                    </div>
                </div>
                <div class="figure-container full-bleed" data-figure-id="fig-04">
                    <div class="figure-content">
                        <img src="images/capture/complete2.png" alt="The Fallback Logic">
                    </div>
                    <div class="caption">
                        The system degrades gracefully from "Auto-Detection" to "Manual Correction" without breaking the user's flow.
                    </div>
                </div>
                <div class="figure-container full-bleed" data-figure-id="fig-05">
                    <div class="figure-content">
                        <img src="images/capture/mockups.png" alt="Production UI">
                    </div>
                    <div class="caption">
                        
                    </div>
                </div>
            </div>
        </div>

        

    </main>

    <script>
    document.addEventListener("DOMContentLoaded", () => {
        const sections = document.querySelectorAll('[data-figure]');
        const figures = document.querySelectorAll('[data-figure-id]');

        if (sections.length && figures.length) {
            figures[0].classList.add('is-active');

            const figureObserver = new IntersectionObserver((entries) => {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        const figureId = entry.target.getAttribute('data-figure');
                        figures.forEach(fig => fig.classList.remove('is-active'));
                        const targetFigure = document.querySelector(`[data-figure-id="${figureId}"]`);
                        if (targetFigure) {
                            targetFigure.classList.add('is-active');
                        }
                    }
                });
            }, {
                rootMargin: '-30% 0px -60% 0px',
                threshold: 0
            });

            sections.forEach(section => figureObserver.observe(section));
        }
    });
</script>
</body>
</html>
